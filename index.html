<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ProbTalk3D produces facial animation for 3D characters based on speech">
  <meta name="keywords" content="ProbTalk3D, VQ-VAE, Animation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ProbTalk3D</h1>
          <h2 class="title is-3 publication-title">Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE</h2>
            <h2 class="title is-3 publication-title">(Accepted at <a href="https://sgmig.hosting.acm.org/mig-2024/">ACM SIGGRAPH MIG 2024)</a></h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block"> <a href="https://wsc462.wixsite.com/scwu"> Sichun Wu </a>, </span>
            <span class="author-block"> <a href="https://www.uu.nl/staff/KIHaque"> Kazi Injamamul Haque </a>, </span>
            <span class="author-block"> <a href="https://www.uu.nl/staff/ZYumak"> Zerrin Yumak </a> </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Utrecht University, </span>
              <span class="author-block">The Netherlands</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-google-scholar"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2409.07966"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              

              <!-- Video Link. -->
              <span class="link-block">
                <a href="#video-container" class="button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/uuembodiedsocialai/ProbTalk3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div id="video-container" class="publication-video">
          <iframe src="./static/videos/ProbTalk3D.mp4"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> <br>
    <!--/ Paper video. -->


    <!-- Methodology -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Methodology </h2>
        <div class="content has-text-justified">
          ProbTalk3D employs a two-stage training approach: first, it learns a motion prior that has a VQ-VAE based autoencoder structure;
          second, it trains a transformer-based neural network for facial motion synthesis conditioned on speech and style,
          leveraging the motion prior acquired in the first stage.
        </div>
          <h3 class="title is-5">Stage-1 Training</h3>
            <div class="content has-text-justified">
              In the first stage, the transformer-based Motion Autoencoder is trained solely on motion data.
              The encoder transfers motion to a latent vector. Subsequently, a discrete codebook is learned through vector quantization to model the data distribution.
              The decoder is trained to reconstruct the input from the quantized vector.
              This process effectively learns a motion prior capable of encoding motion input into a meaningful latent representation.
            </div>
            <div class="methodology-image">
            <img src="./static/images/stage1.png"> </img>
            </div> <br>

          <h3 class="title is-5">Stage-2 Training</h3>
            <div class="content has-text-justified">
              In the second stage, we incorporate an audio encoder and train the model on paired audio-motion data.
              The Audio Encoder is transformer-based and includes a pre-trained HuBERT model. During this stage, the parameters of the Motion Autoencoder are frozen.
              Given an audio input, HuBERT extracts audio features, which are then processed to be mapped to the motion latent space.
              This mapping is conditional on the Style Vector, which allows the model to learn different speaking styles, emotional expressions, and emotion intensities.
            </div>
            <div class="methodology-image">
            <img src="./static/images/stage2.png"> </img>
            </div><br>

          <h3 class="title is-5">Inference</h3>
            <div class="content has-text-justified">
              During inference, the Motion Encoder is excluded, and the remaining components are used to generate facial animation from audio input.
              To introduce non-determinism into the model's outputs, we employ stochastic sampling in the quantization process.
              Specifically, we transform the distance between input vectors and codebook embeddings into probabilities, enabling us to sample codebook embeddings based on them.
            </div>
            <div class="methodology-image">
            <img src="./static/images/inference.png"> </img>
            </div>
      </div>
    </div>
      </div>
    </div>
    <!--/ Methodology -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre>
      <code>
      @inproceedings{Probtalk3D_Wu_MIG24,
        author = {Wu, Sichun and Haque,  Kazi Injamamul and Yumak,  Zerrin},
        title = {ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE},
        booktitle = {The 16th ACM SIGGRAPH Conference on Motion, Interaction, and Games (MIG '24), November 21--23, 2024, Arlington, VA, USA},
        year = {2024},
        location = {Arlington, VA, USA},
        numpages = {12},
        url = {https://doi.org/10.1145/3677388.3696320},
        doi = {10.1145/3677388.3696320},
        publisher = {ACM},
        address = {New York, NY, USA}
        }
      </code>
    </pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website based on the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
            If you want to reuse their source code, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
